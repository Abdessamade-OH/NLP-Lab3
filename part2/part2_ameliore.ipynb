{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e92e6e",
   "metadata": {},
   "source": [
    "<h3>Cleaning and Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e757924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af60c10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       entity sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "\n",
       "                                             content  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the column names\n",
    "column_names = ['tweet_id', 'entity', 'sentiment', 'content']\n",
    "\n",
    "dataset_path = 'twitter_training.csv' \n",
    "#eval_data_path = 'twitter_evaluation.csv'\n",
    "tweets_df = pd.read_csv(dataset_path, header=None, names=column_names)\n",
    "#eval_df = pd.read_csv(dataset_path, header=None, names=column_names)\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e2da86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "null values:  tweet_id       0\n",
      "entity         0\n",
      "sentiment      0\n",
      "content      686\n",
      "dtype: int64\n",
      "duplicates:  2700\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \")\n",
    "\n",
    "print(\"null values: \", tweets_df.isnull().sum())\n",
    "print(\"duplicates: \", tweets_df.duplicated().sum())\n",
    "\n",
    "tweets_df.dropna(inplace=True)\n",
    "tweets_df.drop_duplicates(inplace=True)\n",
    "tweets_df.drop('tweet_id', axis=1, inplace=True)\n",
    "\n",
    "#for evaluation data\n",
    "#eval_df.dropna(inplace=True)\n",
    "#eval_df.drop_duplicates(inplace=True)\n",
    "#eval_df.drop('tweet_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9630ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        entity sentiment                                            content  \\\n",
      "0  Borderlands  Positive  im getting on borderlands and i will murder yo...   \n",
      "1  Borderlands  Positive  I am coming to the borders and I will kill you...   \n",
      "2  Borderlands  Positive  im getting on borderlands and i will kill you ...   \n",
      "3  Borderlands  Positive  im coming on borderlands and i will murder you...   \n",
      "4  Borderlands  Positive  im getting on borderlands 2 and i will murder ...   \n",
      "\n",
      "                     cleaned_text  \n",
      "0    im getting borderland murder  \n",
      "1              coming border kill  \n",
      "2      im getting borderland kill  \n",
      "3     im coming borderland murder  \n",
      "4  im getting borderland 2 murder  \n"
     ]
    }
   ],
   "source": [
    "# Remove Non-String\n",
    "def filter_non_string(df, column):\n",
    "    df = df.dropna(subset=[column])\n",
    "    df[column] = df[column].astype(str)\n",
    "    return df\n",
    "\n",
    "# Convert In LowerCase\n",
    "def normalize_text(text):\n",
    "    \"\"\"Convert text to lowercase to ensure consistency across the corpus.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "# Remove HTML Tags\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text.\"\"\"\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "# Remove URL Or HyperLink\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs or hyperlinks from the text.\"\"\"\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation marks from the text.\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Split Text In Token\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Split the text into individual words or tokens.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Eliminate Stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Eliminate common stopwords from the tokenized text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Remove Emojis\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from the text.\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Stem Tokens\n",
    "def stem_tokens(tokens):\n",
    "    \"\"\"Apply stemming to the tokenized text.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Lemmatize Tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Apply lemmatization to the tokenized text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def preprocess_text(df):\n",
    "    df = filter_non_string(df, 'content')\n",
    "    df['cleaned_text'] = df['content'].apply(normalize_text)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_html_tags)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_urls)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_punctuation)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_emojis)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(tokenize_text)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "    #df['cleaned_text'] = df['cleaned_text'].apply(stem_tokens)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lemmatize_tokens)\n",
    "    # Optionally, join the tokens back into a string if needed\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "data_processed = preprocess_text(tweets_df)\n",
    "#eval_processed = preprocess_text(eval_df)\n",
    "print(data_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5746da21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderland 2 murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "      <td>realized window partition mac like 6 year behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "      <td>realized mac window partition 6 year behind nv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "      <td>realized window partition mac 6 year behind nv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "      <td>realized window partition mac like 6 year behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "      <td>like window partition mac like 6 year behind d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71656 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            entity sentiment  \\\n",
       "0      Borderlands  Positive   \n",
       "1      Borderlands  Positive   \n",
       "2      Borderlands  Positive   \n",
       "3      Borderlands  Positive   \n",
       "4      Borderlands  Positive   \n",
       "...            ...       ...   \n",
       "74677       Nvidia  Positive   \n",
       "74678       Nvidia  Positive   \n",
       "74679       Nvidia  Positive   \n",
       "74680       Nvidia  Positive   \n",
       "74681       Nvidia  Positive   \n",
       "\n",
       "                                                 content  \\\n",
       "0      im getting on borderlands and i will murder yo...   \n",
       "1      I am coming to the borders and I will kill you...   \n",
       "2      im getting on borderlands and i will kill you ...   \n",
       "3      im coming on borderlands and i will murder you...   \n",
       "4      im getting on borderlands 2 and i will murder ...   \n",
       "...                                                  ...   \n",
       "74677  Just realized that the Windows partition of my...   \n",
       "74678  Just realized that my Mac window partition is ...   \n",
       "74679  Just realized the windows partition of my Mac ...   \n",
       "74680  Just realized between the windows partition of...   \n",
       "74681  Just like the windows partition of my Mac is l...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "0                           im getting borderland murder  \n",
       "1                                     coming border kill  \n",
       "2                             im getting borderland kill  \n",
       "3                            im coming borderland murder  \n",
       "4                         im getting borderland 2 murder  \n",
       "...                                                  ...  \n",
       "74677  realized window partition mac like 6 year behi...  \n",
       "74678  realized mac window partition 6 year behind nv...  \n",
       "74679  realized window partition mac 6 year behind nv...  \n",
       "74680  realized window partition mac like 6 year behi...  \n",
       "74681  like window partition mac like 6 year behind d...  \n",
       "\n",
       "[71656 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddf09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the 'Entity' feature\n",
    "data_processed['entity'] = le.fit_transform(data_processed['entity'])\n",
    "#eval_processed['entity'] = le.fit_transform(eval_processed['entity'])\n",
    "\n",
    "# Encode the 'Sentiment' target variable\n",
    "data_processed['sentiment'] = le.fit_transform(data_processed['sentiment'])\n",
    "#eval_processed['sentiment'] = le.fit_transform(eval_processed['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7426be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderland 2 murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity  sentiment                                            content  \\\n",
       "0       4          3  im getting on borderlands and i will murder yo...   \n",
       "1       4          3  I am coming to the borders and I will kill you...   \n",
       "2       4          3  im getting on borderlands and i will kill you ...   \n",
       "3       4          3  im coming on borderlands and i will murder you...   \n",
       "4       4          3  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                     cleaned_text  \n",
       "0    im getting borderland murder  \n",
       "1              coming border kill  \n",
       "2      im getting borderland kill  \n",
       "3     im coming borderland murder  \n",
       "4  im getting borderland 2 murder  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b85ebda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the preprocessed data\n",
    "data_processed.to_csv('preprocessed_tweets.csv', index=False)\n",
    "#eval_processed.to_csv('processed_evaluation_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab498fc1",
   "metadata": {},
   "source": [
    "<h3>Encoding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccb1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries for encoding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e76954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderland 2 murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity  sentiment                                            content  \\\n",
       "0       4          3  im getting on borderlands and i will murder yo...   \n",
       "1       4          3  I am coming to the borders and I will kill you...   \n",
       "2       4          3  im getting on borderlands and i will kill you ...   \n",
       "3       4          3  im coming on borderlands and i will murder you...   \n",
       "4       4          3  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                     cleaned_text  \n",
       "0    im getting borderland murder  \n",
       "1              coming border kill  \n",
       "2      im getting borderland kill  \n",
       "3     im coming borderland murder  \n",
       "4  im getting borderland 2 murder  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the preprocessed data\n",
    "preprocessed_df = pd.read_csv('preprocessed_tweets.csv')\n",
    "#eval_processed = pd.read_csv('processed_evaluation_tweets.csv')\n",
    "\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef8a387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vectors Shape: (71656, 36479)\n",
      "TF-IDF Vectors Shape: (71656, 36479)\n",
      "Word2Vec CBOW Vectors Shape: (71656, 100)\n",
      "Word2Vec Skip-Gram Vectors Shape: (71656, 100)\n"
     ]
    }
   ],
   "source": [
    "#Bag of words\n",
    "def encode_bow(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_vectors = vectorizer.fit_transform(corpus)\n",
    "    return bow_vectors, vectorizer\n",
    "#TF-IDF\n",
    "def encode_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_vectors, vectorizer\n",
    "# Word2Vec (CBOW and Skip-Gram)\n",
    "def train_word2vec(corpus, vector_size=100, window=5, min_count=1, sg=0):\n",
    "    tokenized_corpus = [doc.split() for doc in corpus]\n",
    "    model = Word2Vec(sentences=tokenized_corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg)\n",
    "    return model\n",
    "\n",
    "def encode_word2vec(model, tokenized_corpus):\n",
    "    vectors = []\n",
    "    for tokens in tokenized_corpus:\n",
    "        vector = sum([model.wv[token] for token in tokens if token in model.wv], start=np.zeros(model.vector_size))\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "preprocessed_df['cleaned_text'].fillna('', inplace=True)\n",
    "\n",
    "# Encode using Bag of Words\n",
    "bow_vectors, bow_vectorizer = encode_bow(data_processed['cleaned_text'])\n",
    "\n",
    "# Encode using TF-IDF\n",
    "tfidf_vectors, tfidf_vectorizer = encode_tfidf(data_processed['cleaned_text'])\n",
    "\n",
    "# Train Word2Vec models (CBOW and Skip-Gram)\n",
    "cbow_model = train_word2vec(data_processed['cleaned_text'], sg=0)\n",
    "skipgram_model = train_word2vec(data_processed['cleaned_text'], sg=1)\n",
    "\n",
    "# Tokenized corpus for Word2Vec encoding\n",
    "tokenized_corpus = [doc.split() for doc in data_processed['cleaned_text']]\n",
    "\n",
    "# Encode using Word2Vec (CBOW)\n",
    "cbow_vectors = encode_word2vec(cbow_model, tokenized_corpus)\n",
    "\n",
    "# Encode using Word2Vec (Skip-Gram)\n",
    "skipgram_vectors = encode_word2vec(skipgram_model, tokenized_corpus)\n",
    "\n",
    "# Display the shapes of the encoded vectors to confirm\n",
    "print(\"BoW Vectors Shape:\", bow_vectors.shape)\n",
    "print(\"TF-IDF Vectors Shape:\", tfidf_vectors.shape)\n",
    "print(\"Word2Vec CBOW Vectors Shape:\", cbow_vectors.shape)\n",
    "print(\"Word2Vec Skip-Gram Vectors Shape:\", skipgram_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75696be",
   "metadata": {},
   "source": [
    "<h3>Model Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c62fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4254f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokenized answers with other features\n",
    "features = np.hstack([cbow_vectors, preprocessed_df[['entity']].values])\n",
    "\n",
    "target_variable = preprocessed_df['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target_variable, test_size=0.2, random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc6b05e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.88178167, -4.41066475, -2.67998901, ..., -7.00514266,\n",
       "        -0.61301113,  2.54021149],\n",
       "       [ 1.47821298,  0.0922516 ,  0.50117174, ...,  1.76177269,\n",
       "        -0.73954082,  1.76144001],\n",
       "       [ 0.45390699, -0.16059101,  1.24430284, ..., -0.25214958,\n",
       "        -0.4659901 ,  0.8960343 ],\n",
       "       ...,\n",
       "       [ 1.54340419,  0.55920236,  1.07455769, ...,  2.91723907,\n",
       "        -2.51497099,  1.47535869],\n",
       "       [ 4.97575417, -1.11072598, -0.26299217, ..., -1.42512081,\n",
       "         1.56929128,  0.04258987],\n",
       "       [ 3.45107031, -0.64883866, -0.033836  , ..., -0.69993597,\n",
       "         0.75943356,  1.23980311]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "636f6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train SVM\n",
    "def train_svm(X_train, y_train, kernel='rbf', C=1.0):\n",
    "    svm = SVC(kernel=kernel, C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    return svm\n",
    "\n",
    "# Function to train Naive Bayes\n",
    "def train_naive_bayes(X_train, y_train):\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    return nb\n",
    "\n",
    "# Function to train Logistic Regression\n",
    "def train_logistic_regression(X_train, y_train):\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    return lr\n",
    "\n",
    "# Function to train AdaBoost\n",
    "def train_adaboost(X_train, y_train, n_estimators=50):\n",
    "    ab = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "    ab.fit(X_train, y_train)\n",
    "    return ab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8291265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.551911805749372\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.17      0.26      2529\n",
      "           1       0.59      0.72      0.65      4383\n",
      "           2       0.58      0.46      0.52      3543\n",
      "           3       0.50      0.69      0.58      3877\n",
      "\n",
      "    accuracy                           0.55     14332\n",
      "   macro avg       0.56      0.51      0.50     14332\n",
      "weighted avg       0.56      0.55      0.53     14332\n",
      "\n",
      "********************\n",
      "Naive Bayes Accuracy: 0.4330867987719788\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.06      0.10      2529\n",
      "           1       0.53      0.47      0.50      4383\n",
      "           2       0.50      0.30      0.37      3543\n",
      "           3       0.37      0.76      0.50      3877\n",
      "\n",
      "    accuracy                           0.43     14332\n",
      "   macro avg       0.47      0.40      0.37     14332\n",
      "weighted avg       0.47      0.43      0.40     14332\n",
      "\n",
      "********************\n",
      "Logistic Regression Accuracy: 0.5182109963717555\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.19      0.26      2529\n",
      "           1       0.57      0.67      0.61      4383\n",
      "           2       0.54      0.42      0.47      3543\n",
      "           3       0.47      0.65      0.55      3877\n",
      "\n",
      "    accuracy                           0.52     14332\n",
      "   macro avg       0.51      0.48      0.48     14332\n",
      "weighted avg       0.51      0.52      0.50     14332\n",
      "\n",
      "********************\n",
      "AdaBoost Accuracy: 0.49323192855149317\n",
      "AdaBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.25      0.32      2529\n",
      "           1       0.53      0.63      0.57      4383\n",
      "           2       0.51      0.42      0.46      3543\n",
      "           3       0.46      0.56      0.51      3877\n",
      "\n",
      "    accuracy                           0.49     14332\n",
      "   macro avg       0.48      0.47      0.47     14332\n",
      "weighted avg       0.49      0.49      0.48     14332\n",
      "\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "nb_model = train_naive_bayes(X_train, y_train)\n",
    "lr_model = train_logistic_regression(X_train, y_train)\n",
    "ab_model = train_adaboost(X_train, y_train)\n",
    "svm_model = train_svm(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "ab_pred = ab_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the models\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "ab_accuracy = accuracy_score(y_test, ab_pred)\n",
    "\n",
    "# Generate the classification reports\n",
    "svm_report = classification_report(y_test, svm_pred)\n",
    "nb_report = classification_report(y_test, nb_pred)\n",
    "lr_report = classification_report(y_test, lr_pred)\n",
    "ab_report = classification_report(y_test, ab_pred)\n",
    "\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"SVM Classification Report:\\n\", svm_report)\n",
    "print('*' * 20)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)\n",
    "print('*' * 20)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", lr_accuracy)\n",
    "print(\"Logistic Regression Classification Report:\\n\", lr_report)\n",
    "print('*' * 20)\n",
    "\n",
    "print(\"AdaBoost Accuracy:\", ab_accuracy)\n",
    "print(\"AdaBoost Classification Report:\\n\", ab_report)\n",
    "print('*' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07453185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Accuracy: 0.7795841473625453\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73      2529\n",
      "           1       0.78      0.86      0.82      4383\n",
      "           2       0.78      0.75      0.76      3543\n",
      "           3       0.76      0.79      0.78      3877\n",
      "\n",
      "    accuracy                           0.78     14332\n",
      "   macro avg       0.78      0.77      0.77     14332\n",
      "weighted avg       0.78      0.78      0.78     14332\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a function to train and evaluate a pipeline\n",
    "def train_and_evaluate(X, y, model):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a pipeline with TF-IDF Vectorizer and the specified model\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "    print('\\n')\n",
    "\n",
    "# Specify the models\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model in models:\n",
    "    train_and_evaluate(X, y, model)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
