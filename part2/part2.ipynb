{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e92e6e",
   "metadata": {},
   "source": [
    "<h3>Cleaning and Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e757924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7af60c10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       entity sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "\n",
       "                                             content  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the column names\n",
    "column_names = ['tweet_id', 'entity', 'sentiment', 'content']\n",
    "\n",
    "dataset_path = 'twitter_training.csv' \n",
    "eval_data_path = 'twitter_evaluation.csv'\n",
    "tweets_df = pd.read_csv(dataset_path, header=None, names=column_names)\n",
    "eval_df = pd.read_csv(dataset_path, header=None, names=column_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27946b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "null values:  tweet_id       0\n",
      "entity         0\n",
      "sentiment      0\n",
      "content      686\n",
      "dtype: int64\n",
      "duplicates:  2700\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \")\n",
    "\n",
    "print(\"null values: \", tweets_df.isnull().sum())\n",
    "print(\"duplicates: \", tweets_df.duplicated().sum())\n",
    "\n",
    "tweets_df.dropna(inplace=True)\n",
    "tweets_df.drop_duplicates(inplace=True)\n",
    "tweets_df.drop('tweet_id', axis=1, inplace=True)\n",
    "\n",
    "#for evaluation data\n",
    "eval_df.dropna(inplace=True)\n",
    "eval_df.drop_duplicates(inplace=True)\n",
    "eval_df.drop('tweet_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4673bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        entity sentiment                                            content  \\\n",
      "0  Borderlands  Positive  im getting on borderlands and i will murder yo...   \n",
      "1  Borderlands  Positive  I am coming to the borders and I will kill you...   \n",
      "2  Borderlands  Positive  im getting on borderlands and i will kill you ...   \n",
      "3  Borderlands  Positive  im coming on borderlands and i will murder you...   \n",
      "4  Borderlands  Positive  im getting on borderlands 2 and i will murder ...   \n",
      "\n",
      "                     cleaned_text  \n",
      "0    im getting borderland murder  \n",
      "1              coming border kill  \n",
      "2      im getting borderland kill  \n",
      "3     im coming borderland murder  \n",
      "4  im getting borderland 2 murder  \n"
     ]
    }
   ],
   "source": [
    "# Remove Non-String\n",
    "def filter_non_string(df, column):\n",
    "    df = df.dropna(subset=[column])\n",
    "    df[column] = df[column].astype(str)\n",
    "    return df\n",
    "\n",
    "# Convert In LowerCase\n",
    "def normalize_text(text):\n",
    "    \"\"\"Convert text to lowercase to ensure consistency across the corpus.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "# Remove HTML Tags\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text.\"\"\"\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "# Remove URL Or HyperLink\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs or hyperlinks from the text.\"\"\"\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation marks from the text.\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Split Text In Token\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Split the text into individual words or tokens.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Eliminate Stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Eliminate common stopwords from the tokenized text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Remove Emojis\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from the text.\"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Stem Tokens\n",
    "def stem_tokens(tokens):\n",
    "    \"\"\"Apply stemming to the tokenized text.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Lemmatize Tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Apply lemmatization to the tokenized text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def preprocess_text(df):\n",
    "    df = filter_non_string(df, 'content')\n",
    "    df['cleaned_text'] = df['content'].apply(normalize_text)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_html_tags)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_urls)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_punctuation)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_emojis)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(tokenize_text)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "    #df['cleaned_text'] = df['cleaned_text'].apply(stem_tokens)\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lemmatize_tokens)\n",
    "    # Optionally, join the tokens back into a string if needed\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "data_processed = preprocess_text(tweets_df)\n",
    "eval_processed = preprocess_text(eval_df)\n",
    "print(data_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2918e33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderland 2 murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "      <td>realized window partition mac like 6 year behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "      <td>realized mac window partition 6 year behind nv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "      <td>realized window partition mac 6 year behind nv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "      <td>realized window partition mac like 6 year behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "      <td>like window partition mac like 6 year behind d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71656 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            entity sentiment  \\\n",
       "0      Borderlands  Positive   \n",
       "1      Borderlands  Positive   \n",
       "2      Borderlands  Positive   \n",
       "3      Borderlands  Positive   \n",
       "4      Borderlands  Positive   \n",
       "...            ...       ...   \n",
       "74677       Nvidia  Positive   \n",
       "74678       Nvidia  Positive   \n",
       "74679       Nvidia  Positive   \n",
       "74680       Nvidia  Positive   \n",
       "74681       Nvidia  Positive   \n",
       "\n",
       "                                                 content  \\\n",
       "0      im getting on borderlands and i will murder yo...   \n",
       "1      I am coming to the borders and I will kill you...   \n",
       "2      im getting on borderlands and i will kill you ...   \n",
       "3      im coming on borderlands and i will murder you...   \n",
       "4      im getting on borderlands 2 and i will murder ...   \n",
       "...                                                  ...   \n",
       "74677  Just realized that the Windows partition of my...   \n",
       "74678  Just realized that my Mac window partition is ...   \n",
       "74679  Just realized the windows partition of my Mac ...   \n",
       "74680  Just realized between the windows partition of...   \n",
       "74681  Just like the windows partition of my Mac is l...   \n",
       "\n",
       "                                            cleaned_text  \n",
       "0                           im getting borderland murder  \n",
       "1                                     coming border kill  \n",
       "2                             im getting borderland kill  \n",
       "3                            im coming borderland murder  \n",
       "4                         im getting borderland 2 murder  \n",
       "...                                                  ...  \n",
       "74677  realized window partition mac like 6 year behi...  \n",
       "74678  realized mac window partition 6 year behind nv...  \n",
       "74679  realized window partition mac 6 year behind nv...  \n",
       "74680  realized window partition mac like 6 year behi...  \n",
       "74681  like window partition mac like 6 year behind d...  \n",
       "\n",
       "[71656 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da1a5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the 'Entity' feature\n",
    "data_processed['entity'] = le.fit_transform(data_processed['entity'])\n",
    "eval_processed['entity'] = le.fit_transform(eval_processed['entity'])\n",
    "\n",
    "# Encode the 'Sentiment' target variable\n",
    "data_processed['sentiment'] = le.fit_transform(data_processed['sentiment'])\n",
    "eval_processed['sentiment'] = le.fit_transform(eval_processed['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e75b3ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderland 2 murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity  sentiment                                            content  \\\n",
       "0       4          3  im getting on borderlands and i will murder yo...   \n",
       "1       4          3  I am coming to the borders and I will kill you...   \n",
       "2       4          3  im getting on borderlands and i will kill you ...   \n",
       "3       4          3  im coming on borderlands and i will murder you...   \n",
       "4       4          3  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                     cleaned_text  \n",
       "0    im getting borderland murder  \n",
       "1              coming border kill  \n",
       "2      im getting borderland kill  \n",
       "3     im coming borderland murder  \n",
       "4  im getting borderland 2 murder  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b85ebda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the preprocessed data\n",
    "data_processed.to_csv('preprocessed_tweets.csv', index=False)\n",
    "eval_processed.to_csv('processed_evaluation_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab498fc1",
   "metadata": {},
   "source": [
    "<h3>Encoding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fccb1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries for encoding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e76954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting borderland 2 murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity  sentiment                                            content  \\\n",
       "0       4          3  im getting on borderlands and i will murder yo...   \n",
       "1       4          3  I am coming to the borders and I will kill you...   \n",
       "2       4          3  im getting on borderlands and i will kill you ...   \n",
       "3       4          3  im coming on borderlands and i will murder you...   \n",
       "4       4          3  im getting on borderlands 2 and i will murder ...   \n",
       "\n",
       "                     cleaned_text  \n",
       "0    im getting borderland murder  \n",
       "1              coming border kill  \n",
       "2      im getting borderland kill  \n",
       "3     im coming borderland murder  \n",
       "4  im getting borderland 2 murder  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the preprocessed data\n",
    "preprocessed_df = pd.read_csv('preprocessed_tweets.csv')\n",
    "eval_processed = pd.read_csv('processed_evaluation_tweets.csv')\n",
    "\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ef8a387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vectors Shape: (71656, 36479)\n",
      "TF-IDF Vectors Shape: (71656, 36479)\n",
      "Word2Vec CBOW Vectors Shape: (71656, 100)\n",
      "Word2Vec Skip-Gram Vectors Shape: (71656, 100)\n"
     ]
    }
   ],
   "source": [
    "#Bag of words\n",
    "def encode_bow(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_vectors = vectorizer.fit_transform(corpus)\n",
    "    return bow_vectors, vectorizer\n",
    "#TF-IDF\n",
    "def encode_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_vectors, vectorizer\n",
    "# Word2Vec (CBOW and Skip-Gram)\n",
    "def train_word2vec(corpus, vector_size=100, window=5, min_count=1, sg=0):\n",
    "    tokenized_corpus = [doc.split() for doc in corpus]\n",
    "    model = Word2Vec(sentences=tokenized_corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg)\n",
    "    return model\n",
    "\n",
    "def encode_word2vec(model, tokenized_corpus):\n",
    "    vectors = []\n",
    "    for tokens in tokenized_corpus:\n",
    "        vector = sum([model.wv[token] for token in tokens if token in model.wv], start=np.zeros(model.vector_size))\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "preprocessed_df['cleaned_text'].fillna('', inplace=True)\n",
    "\n",
    "# Encode using Bag of Words\n",
    "bow_vectors, bow_vectorizer = encode_bow(data_processed['cleaned_text'])\n",
    "\n",
    "# Encode using TF-IDF\n",
    "tfidf_vectors, tfidf_vectorizer = encode_tfidf(data_processed['cleaned_text'])\n",
    "\n",
    "# Train Word2Vec models (CBOW and Skip-Gram)\n",
    "cbow_model = train_word2vec(data_processed['cleaned_text'], sg=0)\n",
    "skipgram_model = train_word2vec(data_processed['cleaned_text'], sg=1)\n",
    "\n",
    "# Tokenized corpus for Word2Vec encoding\n",
    "tokenized_corpus = [doc.split() for doc in data_processed['cleaned_text']]\n",
    "\n",
    "# Encode using Word2Vec (CBOW)\n",
    "cbow_vectors = encode_word2vec(cbow_model, tokenized_corpus)\n",
    "\n",
    "# Encode using Word2Vec (Skip-Gram)\n",
    "skipgram_vectors = encode_word2vec(skipgram_model, tokenized_corpus)\n",
    "\n",
    "# Display the shapes of the encoded vectors to confirm\n",
    "print(\"BoW Vectors Shape:\", bow_vectors.shape)\n",
    "print(\"TF-IDF Vectors Shape:\", tfidf_vectors.shape)\n",
    "print(\"Word2Vec CBOW Vectors Shape:\", cbow_vectors.shape)\n",
    "print(\"Word2Vec Skip-Gram Vectors Shape:\", skipgram_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75696be",
   "metadata": {},
   "source": [
    "<h3>Model Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c62fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df3c0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = data_processed['content']\n",
    "y = data_processed['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15630e63",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.7 GiB for an array with shape (57324, 29718) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ab\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train models using Bag of Words\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m nb_model_bow \u001b[38;5;241m=\u001b[39m train_naive_bayes(X_train_bow, y_train)\n\u001b[0;32m     30\u001b[0m svm_model_bow \u001b[38;5;241m=\u001b[39m train_svm(X_train_bow, y_train)\n\u001b[0;32m     31\u001b[0m lr_model_bow \u001b[38;5;241m=\u001b[39m train_logistic_regression(X_train_bow, y_train)\n",
      "Cell \u001b[1;32mIn[78], line 10\u001b[0m, in \u001b[0;36mtrain_naive_bayes\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_naive_bayes\u001b[39m(X_train, y_train):\n\u001b[0;32m      9\u001b[0m     nb \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[1;32m---> 10\u001b[0m     nb\u001b[38;5;241m.\u001b[39mfit(X_train\u001b[38;5;241m.\u001b[39mtoarray(), y_train)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:263\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_fit(\n\u001b[0;32m    264\u001b[0m     X, y, np\u001b[38;5;241m.\u001b[39munique(y), _refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[0;32m    265\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:431\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    425\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# If the ratio of data variance between dimensions is too small, it\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# will cause numerical errors. To address this, we artificially\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# boost the variance by epsilon, a small fraction of the standard\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# deviation of the largest dimension.\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_smoothing \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_call:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;66;03m# This is the first call to partial_fit:\u001b[39;00m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# initialize various cumulative counters\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvar\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3747\u001b[0m, in \u001b[0;36mvar\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m   3744\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3745\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m var(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_var(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof,\n\u001b[0;32m   3748\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:236\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    231\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(arr \u001b[38;5;241m-\u001b[39m arrmean)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m    239\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.7 GiB for an array with shape (57324, 29718) and data type float64"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Encode using Bag of Words\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# Define model training functions\n",
    "def train_naive_bayes(X_train, y_train):\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train.toarray(), y_train)\n",
    "    return nb\n",
    "\n",
    "def train_svm(X_train, y_train, kernel='rbf', C=1.0):\n",
    "    svm = SVC(kernel=kernel, C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    return svm\n",
    "\n",
    "def train_logistic_regression(X_train, y_train):\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    return lr\n",
    "\n",
    "def train_adaboost(X_train, y_train, n_estimators=50):\n",
    "    ab = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "    ab.fit(X_train, y_train)\n",
    "    return ab\n",
    "\n",
    "# Train models using Bag of Words\n",
    "nb_model_bow = train_naive_bayes(X_train_bow, y_train)\n",
    "svm_model_bow = train_svm(X_train_bow, y_train)\n",
    "lr_model_bow = train_logistic_regression(X_train_bow, y_train)\n",
    "ab_model_bow = train_adaboost(X_train_bow, y_train)\n",
    "\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report\n",
    "\n",
    "# Evaluate models using Bag of Words\n",
    "print(\"Evaluating models using Bag of Words...\\n\")\n",
    "nb_accuracy_bow, nb_report_bow = evaluate_model(nb_model_bow, X_test_bow, y_test)\n",
    "print(f\"Naive Bayes Accuracy (BoW): {nb_accuracy_bow}\")\n",
    "print(f\"Naive Bayes Classification Report (BoW):\\n{nb_report_bow}\")\n",
    "\n",
    "svm_accuracy_bow, svm_report_bow = evaluate_model(svm_model_bow, X_test_bow, y_test)\n",
    "print(f\"SVM Accuracy (BoW): {svm_accuracy_bow}\")\n",
    "print(f\"SVM Classification Report (BoW):\\n{svm_report_bow}\")\n",
    "\n",
    "lr_accuracy_bow, lr_report_bow = evaluate_model(lr_model_bow, X_test_bow, y_test)\n",
    "print(f\"Logistic Regression Accuracy (BoW): {lr_accuracy_bow}\")\n",
    "print(f\"Logistic Regression Classification Report (BoW):\\n{lr_report_bow}\")\n",
    "\n",
    "ab_accuracy_bow, ab_report_bow = evaluate_model(ab_model_bow, X_test_bow, y_test)\n",
    "print(f\"AdaBoost Accuracy (BoW): {ab_accuracy_bow}\")\n",
    "print(f\"AdaBoost Classification Report (BoW):\\n{ab_report_bow}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc8ca97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7795841473625453\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73      2529\n",
      "           1       0.78      0.86      0.82      4383\n",
      "           2       0.78      0.75      0.76      3543\n",
      "           3       0.76      0.79      0.78      3877\n",
      "\n",
      "    accuracy                           0.78     14332\n",
      "   macro avg       0.78      0.77      0.77     14332\n",
      "weighted avg       0.78      0.78      0.78     14332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assuming 'data_processed' is your DataFrame and 'Sentiment' is the target variable\n",
    "\n",
    "# Join the words in the 'Tweet content' column\n",
    "#data_processed['content'] = data_processed['content'].apply(' '.join)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data_processed['content']\n",
    "y = data_processed['sentiment']\n",
    "\n",
    "# Create a pipeline with TF-IDF Vectorizer and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b59d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Accuracy: 0.7795841473625453\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.73      2529\n",
      "           1       0.78      0.86      0.82      4383\n",
      "           2       0.78      0.75      0.76      3543\n",
      "           3       0.76      0.79      0.78      3877\n",
      "\n",
      "    accuracy                           0.78     14332\n",
      "   macro avg       0.78      0.77      0.77     14332\n",
      "weighted avg       0.78      0.78      0.78     14332\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a function to train and evaluate a pipeline\n",
    "def train_and_evaluate(X, y, model):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a pipeline with TF-IDF Vectorizer and the specified model\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "    print('\\n')\n",
    "\n",
    "# Specify the models\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    SVC(),\n",
    "    MultinomialNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model in models:\n",
    "    train_and_evaluate(X, y, model)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
